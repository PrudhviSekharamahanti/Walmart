{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "infectious-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "white-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'p5CzEB9O7j6uS4pgoWGpG28oe'\n",
    "consumer_secret= 'Q8lMvPsnuKFJ7WmCRBbX2ZuIyD6kCg21oHjnE41kAXWv9V0FmN'\n",
    "access_token= '1097726291065954304-ltQ5XhlGij1Tq90YtQdli9u1N4y0j6'\n",
    "access_token_secret= 'GYmiQKmmcLk1xwc1M5yQxs9i9I7uMBNMVqfaVJF9GvDFh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "historical-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAPrxMQEAAAAArdIifiX3DO3CXHsKN33dVku4TGw%3DftFN1DlKV5HNLKmKB7KFwEfiPONZqx9oRt8AfAtGkyjV48bsN1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-deviation",
   "metadata": {},
   "source": [
    "### Scenario 1: Real-time streaming\n",
    "\n",
    "***Note:*** The run has been terminated after extracting two tweets since this is real-time continuous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "lesbian-perfume",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [{\"id\": \"1454882244762509312\", \"value\": \"Justin Beiber\", \"tag\": \"music\"}], \"meta\": {\"sent\": \"2021-10-31T19:01:34.815Z\", \"result_count\": 1}}\n",
      "{\"meta\": {\"sent\": \"2021-10-31T19:01:36.053Z\", \"summary\": {\"deleted\": 1, \"not_deleted\": 0}}}\n",
      "{\"data\": [{\"value\": \"Justin Beiber\", \"tag\": \"music\", \"id\": \"1454886289078108168\"}], \"meta\": {\"sent\": \"2021-10-31T19:01:37.757Z\", \"summary\": {\"created\": 1, \"not_created\": 0, \"valid\": 1, \"invalid\": 0}}}\n",
      "200\n",
      "{\n",
      "    \"data\": {\n",
      "        \"id\": \"1454886797197021191\",\n",
      "        \"text\": \"AAAAH JUSTIN BEIBER\"\n",
      "    },\n",
      "    \"matching_rules\": [\n",
      "        {\n",
      "            \"id\": \"1454886289078108168\",\n",
      "            \"tag\": \"music\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": {\n",
      "        \"id\": \"1454887602167992328\",\n",
      "        \"text\": \"@OldMemeArchive F**K YOU JUSTIN BEIBER\"\n",
      "    },\n",
      "    \"matching_rules\": [\n",
      "        {\n",
      "            \"id\": \"1454886289078108168\",\n",
      "            \"tag\": \"music\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-c79a68fe65a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-c79a68fe65a1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mdelete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelete_all_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mget_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-c79a68fe65a1>\u001b[0m in \u001b[0;36mget_stream\u001b[0;34m(set)\u001b[0m\n\u001b[1;32m     70\u001b[0m             )\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mresponse_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36miter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mpending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_unicode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpending\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
    "    return r\n",
    "\n",
    "def get_rules():\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def delete_all_rules(rules):\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        auth=bearer_oauth,\n",
    "        json=payload\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot delete rules (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def set_rules(delete):\n",
    "    # You can adjust the rules if needed\n",
    "    sample_rules = [\n",
    "        {\"value\": \"Justin Beiber\", \"tag\": \"music\"}\n",
    "    ]\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        auth=bearer_oauth,\n",
    "        json=payload,\n",
    "    )\n",
    "    if response.status_code != 201:\n",
    "        raise Exception(\n",
    "            \"Cannot add rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def get_stream(set):\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream\", auth=bearer_oauth, stream=True\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get stream (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    for response_line in response.iter_lines():\n",
    "        if response_line:\n",
    "            json_response = json.loads(response_line)\n",
    "            \n",
    "            #Loading to Bigquey\n",
    "            \n",
    "            print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "            df['id']=[json_response['data']['id']]\n",
    "            df['text']=[json_response['data']['text']]\n",
    "            #Extracting current timestamp\n",
    "            currentdt=datetime.now()\n",
    "            df['load_date_time']=[currentdt]\n",
    "\n",
    "            #Creating unique batch id\n",
    "            id = uuid.uuid1()\n",
    "            df['BatchId']=[id.int]\n",
    "            df.to_gbq('myproject-330700.twitter_data.tweet_data', project_id='myproject-330700',credentials=credentials,\n",
    "                                  if_exists='append')\n",
    "\n",
    "\n",
    "def main():\n",
    "    rules = get_rules()\n",
    "    delete = delete_all_rules(rules)\n",
    "    set = set_rules(delete)\n",
    "    get_stream(set)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-compatibility",
   "metadata": {},
   "source": [
    "### Loading Stream into Bigquery\n",
    "\n",
    "![tweet data](tweet_stream_load.png)\n",
    "\n",
    "\n",
    "***Since this is real-time load into bigquery we cannot capture unique count/total count. Those were captured as part of Batch Load Scenario Below***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-break",
   "metadata": {},
   "source": [
    "### Scenario 2: Batch Load\n",
    "\n",
    "***Note:*** The free account which has been utilized for this exercise will extract all tweets related to keyword for past 7 days from a given date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adjusted-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "threatened-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "employed-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "word='justin beiber #music'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "japanese-seven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.cursor.ItemIterator at 0x7f94433cb760>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweepy.Cursor(api.search_tweets,\n",
    "              q=word,\n",
    "              lang=\"en\",\n",
    "              until=\"2021-10-30\").items()\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "closing-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[tweet.id,tweet.created_at,tweet.text] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "missing-machinery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1453371085563572230,\n",
       "  datetime.datetime(2021, 10, 27, 14, 40, 43, tzinfo=datetime.timezone.utc),\n",
       "  '#NowPlaying: Runaway Love (Remix) (Feat. Raekwon &amp; Kanye West) by Justin Beiber | Tune in to #SexyBlackRadio (link… https://t.co/5WQ3dd9506'],\n",
       " [1452358382699044871,\n",
       "  datetime.datetime(2021, 10, 24, 19, 36, 36, tzinfo=datetime.timezone.utc),\n",
       "  'Arianna Grande\\nThe Weeknd\\nDrake\\nJeremih\\nPolo G\\nThe Kid Laroi\\nPost Malone\\nJustin Beiber\\nDJ Khaled\\n\\n🙏🙏🙏🙏🙏🙏🙏 #music']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "small-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "ids=[data[i][0] for i in range(0,len(data))]\n",
    "text=[data[i][1] for i in range(0,len(data))]\n",
    "\n",
    "df['id']=ids\n",
    "df['text']=text\n",
    "\n",
    "#count of all tweets extracted\n",
    "twcount=len(df)\n",
    "#count of unique tweets extracted\n",
    "twuncount=len(df.id.unique())\n",
    "\n",
    "#Dropping duplicates\n",
    "df.drop_duplicates(subset =\"id\",keep = False, inplace = True)\n",
    "\n",
    "#Extracting current timestamp\n",
    "currentdt=datetime.now()\n",
    "df['load_date_time']=currentdt\n",
    "\n",
    "#Creating unique batch id\n",
    "id = uuid.uuid1()\n",
    "df['BatchId']=id.int\n",
    "\n",
    "#Creating log table\n",
    "log_df=pd.DataFrame()\n",
    "log_df['BatchId']=[id.int]\n",
    "log_df['tweet_count']=[twcount]\n",
    "log_df['uniq_tweet_count']=[twuncount]\n",
    "log_df['load_date_time']=[currentdt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-opening",
   "metadata": {},
   "source": [
    "### Loading to Google Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "greek-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "service_account_info = json.load(open('/Users/prudhvichandra/Documents/twitter_gcp_scacc.json'))\n",
    "credentials = service_account.Credentials.from_service_account_info(\n",
    "    service_account_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "smoking-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "df.to_gbq('myproject-330700.twitter_data.tweet_data', project_id='myproject-330700',credentials=credentials,\n",
    "          if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "corrected-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.37s/it]\n"
     ]
    }
   ],
   "source": [
    "log_df.to_gbq('myproject-330700.twitter_data.tweeet_log', project_id='myproject-330700',credentials=credentials,\n",
    "          if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-variation",
   "metadata": {},
   "source": [
    "### Link to access tweet data on bigquery \n",
    "\n",
    "![tweet data](tweet_data.png)\n",
    "\n",
    "### Link to access log table of tweet Load\n",
    "\n",
    "![tweet data](tweet_log.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-stylus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "touched-anderson",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "***What are the risks involved in building such a pipeline?***\n",
    "\n",
    "- Need to have effecient failure handling mechanisms otherwise there is high chance of missing data or having duplicated data\n",
    "- The API token, keys and urls has to closely monitored and have to updated when there is a change\n",
    "- Error traceability is an important aspect of such pipelines\n",
    "\n",
    "***How would you roll out the pipeline going from proof-of-concept to a production-ready solution?***\n",
    "\n",
    "Inorder to roll out a production ready solution we need to determine the following:\n",
    "\n",
    "- Agree upon how frequently we need this data to be stored(real-time/Batch) based on business requirement.\n",
    "- Once agreed upon we have to collaborate with API teams and make sure we have all the production ready urls and keys required\n",
    "- Then we need to design a Data Architecture and define what components or services are needed to accomplish this\n",
    "    - API ----> App running on Google Cloud Run/VM to stream data from api -----> Load into Bigquery(Real-time)\n",
    "    - API ----> Python code on Google Cloud Functions that can be scheduled using pubsub & Cloud Scheduler -------> Loading into Bigquery(Batch)\n",
    "- Once we have data architecture finalised we define table structures/schemas and access controls etc.\n",
    "- The we need to parameterise the Python code for example storing the keys, tokens, table names, urls in config files/env variables and read from there instead of hard coding. Shell scripting etc.\n",
    "- Then we make sure we enable each service required and build a end-to-end pipeline on dev\n",
    "- Testing the pipeline\n",
    "- UAT approval\n",
    "- Moving elements to production\n",
    "\n",
    "\n",
    "***What would a production-ready solution entail that a POC wouldn't?***\n",
    "\n",
    "- No defined Architecture\n",
    "- Everything is hard-coded in POC, no config files, no shell sripts, no env variables\n",
    "- No enabling systems to run the code\n",
    "- No users are involved in POC stage\n",
    "    \n",
    "    \n",
    "***What is the level of effort required to deliver each phase of the solution?***\n",
    "\n",
    "- Intial Business Analysis with users/stakeholders to define product owners and agree upon the requirements\n",
    "- Data Modelling/Architecture to design an architure and other enabling elements \n",
    "- Cloud Infrasture setup and code finalizations.\n",
    "- Engineering data pipeline and making sure we have the business use case justified\n",
    "- Testing the pipeline\n",
    "- User acceptance and approval\n",
    "- Deploying the pipeline into production\n",
    "\n",
    "    \n",
    "***What is your estimated timeline for delivery for a production-ready solution?***\n",
    "\n",
    "- Businees Analysis and complete requirement handling: 1 week\n",
    "- Data Modelling/ Architecture: 1 week\n",
    "- Cloud Infrasture setup and code enablement: 2 weeks\n",
    "- Testing: 1 week\n",
    "- UAT & Deployment: 1 Week\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-privilege",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
